{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#data set link : https://www.kaggle.com/datasets/elvinagammed/chatbots-intent-recognition-dataset"
      ],
      "metadata": {
        "id": "o9LgP4qYJWx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CuCbzvKfJWja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILwC-eqaSD48"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from google.colab import drive\n",
        "import json\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/supervisedMLKaggle')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Z1WKm5AjLgF"
      },
      "outputs": [],
      "source": [
        "# Opening JSON file\n",
        "f = open('Intent.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "data = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBpNWuCXjVI3"
      },
      "outputs": [],
      "source": [
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeWm3hWMVRmH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlExzaloiWft"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import nltk\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM , Dense,GlobalAveragePooling1D,Flatten, Dropout , GRU\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Conv1D, MaxPool1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AvXVi5y3-Xl"
      },
      "source": [
        "Lire notre Dataset comme étant un fichier JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZXWyX8GoBaO"
      },
      "outputs": [],
      "source": [
        "for key in data['intents']:\n",
        "  print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWHQu2tiKbHV"
      },
      "outputs": [],
      "source": [
        "data['intents'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd9Q3p71KgQ8"
      },
      "outputs": [],
      "source": [
        "type(data['intents'][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['intents'][0].get('intent')"
      ],
      "metadata": {
        "id": "F6OacloFLdMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNezfyGlLbKq"
      },
      "outputs": [],
      "source": [
        "print(data['intents'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwycHc_ZLjbw"
      },
      "outputs": [],
      "source": [
        "(data['intents'][0]).get('intent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFHAbfqZjaE6"
      },
      "outputs": [],
      "source": [
        "def intent_texte(data):\n",
        "  intents = []\n",
        "  texts = []\n",
        "  for ligne in data['intents']:\n",
        "    intent= ligne.get('intent')\n",
        "    if 'text' in ligne:\n",
        "      for texte in ligne['text']:\n",
        "          print(texte)\n",
        "          texte=texte\n",
        "          print('intent is ', intent)\n",
        "          intent=intent\n",
        "          intents.append(intent)\n",
        "          texts.append(texte)\n",
        "  return intents, texts\n",
        "#s(data['intents'][0]).get('intent')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGB7Q0lq5_kk"
      },
      "outputs": [],
      "source": [
        "h, f=intent_texte(data)\n",
        "print(h, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcH4YHNrkQJN"
      },
      "outputs": [],
      "source": [
        "dataset1=pd.DataFrame({\"inputs\":f , \"tags\": h})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjq17pPu4ir3"
      },
      "outputs": [],
      "source": [
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bpyS8np6DVV"
      },
      "outputs": [],
      "source": [
        "dataset1.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeNkFiI_6Vo2"
      },
      "outputs": [],
      "source": [
        "dataset1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIyaD9wn6idw"
      },
      "outputs": [],
      "source": [
        "dataset1['inputs']=dataset1['inputs'].apply(lambda x : x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDmWzVmc9AeS"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1['inputs']"
      ],
      "metadata": {
        "id": "pkzPuOSZMLc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7l2RThw7wnW"
      },
      "outputs": [],
      "source": [
        "#on convertit les caractères en miniscule seulement si le caractère n'est pas dans la chaine de ponctuation\n",
        "dataset1['inputs']= dataset1['inputs'].apply(lambda x : [caractere for caractere in x if caractere not in string.punctuation] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiSFPTbu73BB"
      },
      "outputs": [],
      "source": [
        "dataset1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqg7O5kJ9DjE"
      },
      "outputs": [],
      "source": [
        "#concaténation des caractères au niveau des lignes de notre dataset\n",
        "dataset1['inputs']= dataset1['inputs'].apply(lambda mot : ''.join(mot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWSW1jZe9dOu"
      },
      "outputs": [],
      "source": [
        "dataset1['inputs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruv2ZHp89ccX"
      },
      "outputs": [],
      "source": [
        "tokenizer= Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(dataset1['inputs']) #mettre à jour le dictionnaire interne du Tokenizer.\n",
        "train = tokenizer.texts_to_sequences(dataset1['inputs'])  #convertir le texte en séquences d'entiers en utilisant le dictionnaire interne du Tokenizer.\n",
        "features = pad_sequences(train)\n",
        "le=LabelEncoder() #convertir les données textuelles en données étiquetées.\n",
        "labels=le.fit_transform(dataset1['tags']) #cette méthode de la classe Label Encoder prends les étiquettes de la colonne 'tags' et les transforme en entiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WexZ7fcnF6lU"
      },
      "outputs": [],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcF9lg61_4UM"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COkujvpDGZ2A"
      },
      "outputs": [],
      "source": [
        "len(features[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQ69BWe4Gdq3"
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGqnju3GiMz"
      },
      "outputs": [],
      "source": [
        "vocabulary = len(tokenizer.word_index)\n",
        "print('number of unique words : ', vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUMPQowDAc52"
      },
      "outputs": [],
      "source": [
        "output_length = le.classes_.shape[0]\n",
        "print('output length : ', output_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzG-yimfArz7"
      },
      "outputs": [],
      "source": [
        "tokenizer.word_index #mappe les mots uniques dans notre corpus avec leurs index, ce qui permet de visualiser la correspondance entre le corpus et son index\n",
        "#on a les mots uniques dans la clé et la valeur c'est l'index  du mot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsPBQ0dYBYBA"
      },
      "source": [
        "BUILD RNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ULm7YGEAuki"
      },
      "outputs": [],
      "source": [
        "m = Sequential() #initialisation d'un modèle de réseau de neurones séquentiel\n",
        "m.add(Input(shape=(features.shape[1]))) #on ajoute une couche d'entrée au modèle\n",
        "m.add(Embedding(vocabulary + 1,100)) #cpuche d'embeding au modèle\n",
        "m.add(Conv1D(filters=32, kernel_size=5, activation=\"relu\", kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_regularizer=tf.keras.regularizers.L2(0.0001), kernel_regularizer=tf.keras.regularizers.L2(0.0001), activity_regularizer = tf.keras.regularizers.L2(0.0001))) # Ajoute une couche de convolution 1D avec 32 filtres et une taille de noyau de 5. Cette couche est suivie d'une fonction d'activation ReLU et de régularisations pour le biais, les poids et l'activité.\n",
        "m.add(Dropout(0.3)) #Ajoute une couche de dropout avec un taux de dropout de 0.3 pour régulariser le modèle et prévenir le surapprentissage.\n",
        "m.add(LSTM(32, dropout=0.3,return_sequences=True)) #Ajoute une couche LSTM avec 32 unités, un taux de dropout de 0.3 et return_sequences=True, ce qui signifie que cette couche renvoie des séquences au lieu d'une seule sortie.\n",
        "m.add(LSTM(16, dropout=0.3,return_sequences=False)) #\n",
        "m.add(Dense(128,activation=\"relu\", activity_regularizer = tf.keras.regularizers.L2(0.0001)))\n",
        "m.add(Dropout(0.6))\n",
        "m.add(Dense(output_length, activation=\"softmax\", activity_regularizer = tf.keras.regularizers.L2(0.0001)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv-QJ_QtYF-K"
      },
      "source": [
        "On utilise ce modèle pour la classification de texte,où les séquences de texte sont représentés sous forme d'entiers.\n",
        "Le modèle utilise des couches d'embedding, de convolution et de LSTM pour extraire des caractéristiques pertinentes avant de les passer à une couche de sortie softmax pour la classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGLd6PVFWusS"
      },
      "outputs": [],
      "source": [
        "! wget https://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIv4DkfOYvA1"
      },
      "outputs": [],
      "source": [
        "m.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xmHryduY0F7",
        "outputId": "db5c6c04-37cb-40c3-8c94-8b03d0cc96eb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mpfWDHEY1o5"
      },
      "outputs": [],
      "source": [
        "glove_dir = \"glove.6B.100d.txt\"\n",
        "embeddings_index = {}\n",
        "file_ = open(glove_dir)\n",
        "for line in file_:\n",
        "    arr = line.split()\n",
        "    single_word = arr[0]\n",
        "    w = np.asarray(arr[1:],dtype='float32')\n",
        "    embeddings_index[single_word] = w\n",
        "file_.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSB78VZWiFAq"
      },
      "outputs": [],
      "source": [
        "max_words= vocabulary + 1\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, 100)).astype(object)\n",
        "for word , i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i]= embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUWftJ15i73V"
      },
      "outputs": [],
      "source": [
        "m.layers[0].set_weights([embedding_matrix])\n",
        "m.layers[0].trainable = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFSXPECPi9_i"
      },
      "outputs": [],
      "source": [
        "m.compile(loss=\"sparse_categorical_crossentropy\",optimizer='adam',metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSNnEHwwi_cl"
      },
      "outputs": [],
      "source": [
        "\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s8EzmZujC82"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import TensorBoard, EarlyStopping\n",
        "earlyStopping = EarlyStopping(monitor = 'loss', patience = 400, mode = 'min', restore_best_weights = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoMgeUfojDsf"
      },
      "outputs": [],
      "source": [
        "history_training = m.fit(features,labels,epochs=2000, batch_size=64, callbacks=[ earlyStopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFzAQLjEjIyd"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "def draw_plot(data, type_data):\n",
        "  mpl.style.use('seaborn')\n",
        "  plt.figure(figsize = (25, 5))\n",
        "  plt.plot(data,'darkorange', label='Train')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(type_data)\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7FxsJtdjOy6"
      },
      "outputs": [],
      "source": [
        "draw_plot(history_training.history['accuracy'],'training set accuracy' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWD2DHTNjREn"
      },
      "outputs": [],
      "source": [
        "draw_plot(history_training.history['loss'],'training set loss' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSuy9zYrjRUj"
      },
      "outputs": [],
      "source": [
        "m.evaluate(features, labels, batch_size = 64)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnDS22JqjTPk"
      },
      "source": [
        "Live Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYHVRpJtjSm5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def generate_answer(query):\n",
        "  texts = []\n",
        "  pred_input = query\n",
        "  pred_input = [letters.lower() for letters in pred_input if letters not in string.punctuation]\n",
        "  pred_input = ''.join(pred_input)\n",
        "  texts.append(pred_input)\n",
        "  pred_input = tokenizer.texts_to_sequences(texts)\n",
        "  pred_input = np.array(pred_input).reshape(-1)\n",
        "  pred_input = pad_sequences([pred_input],input_shape)\n",
        "  output = m.predict(pred_input)\n",
        "  output = output.argmax()\n",
        "  response_tag = le.inverse_transform([output])[0]\n",
        "  return random.choice(responses[response_tag])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gHstaKaj4qx"
      },
      "outputs": [],
      "source": [
        "list_que = [\"hello\", \"i am kaled\",\"what is my name?\",\n",
        "            \"what is your name?\", \"tell me please, what is your name?\"]\n",
        "for i in list_que:\n",
        "  print(\"you: {}\".format(i))\n",
        "  res_tag = generate_answer(i)\n",
        "  print(res_tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T5qNH-_mp7x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}